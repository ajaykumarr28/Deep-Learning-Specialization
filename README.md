# Deep-Learning-Specialization

This repository contains my solutions to the programming assignments given by [deeplearning.ai](https://www.deeplearning.ai/) as part of their [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning).

1. [Neural Networks and Deep Learning](https://github.com/ajaykumarr28/Deep-Learning-Specialization/tree/master/1%20-%20Neural%20Networks%20and%20Deep%20Learning) - In this course, I was able to learn about the foundations of deep learning. After finishing this class, I was able to:
- Understand the major technology trends driving Deep Learning
- Be able to build, train and apply fully connected deep neural networks
- Know how to implement efficient (vectorized) neural networks
- Understand the key parameters in a neural network's architecture

  Assignments - 

    Week 2 - [Logistic Regression with a Neural Network mindset](https://github.com/ajaykumarr28/Deep-Learning-Specialization/blob/master/1%20-%20Neural%20Networks%20and%20Deep%20Learning/Logistic%20Regression%20with%20a%20Neural%20Network%20mindset.ipynb) - I built a logistic regression classifier to recognize cats. This assignment walked me through on how to do this with a Neural Network mindset, and also honed my intuitions about deep learning.

    Week 3 - [Planar data classification with one hidden layer](https://github.com/ajaykumarr28/Deep-Learning-Specialization/blob/master/1%20-%20Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb) - In this assignment, I built my first neural network, which had a single hidden layer, and I saw a big difference between this model and the one I implemented using logistic regression.

    Week 4 - [(i) Building your Deep Neural Network - Step by Step](https://github.com/ajaykumarr28/Deep-Learning-Specialization/blob/master/1%20-%20Neural%20Networks%20and%20Deep%20Learning/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step.ipynb) - In this notebook, I implemented all the functions required to build a deep neural network.

    Week 4 - [(ii) Deep Neural Network - Application](https://github.com/ajaykumarr28/Deep-Learning-Specialization/blob/master/1%20-%20Neural%20Networks%20and%20Deep%20Learning/Deep%20Neural%20Network%20-%20Application.ipynb) - I used the functions I implemented in the previous assignment to build a deep network, and apply it to cat vs non-cat classification and saw a drastic improvement in accuracy relative to my earlier logistic regression implementation.

2. [Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization](https://github.com/ajaykumarr28/Deep-Learning-Specialization/tree/master/2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization) - This course taught me the "magic" of getting deep learning to work well. Rather than the deep learning process being a black box, I understood what drives performance, and was able to get good results more systematically. I also got to learn TensorFlow. After this course, I was able to:
- Understand industry best-practices for building deep learning applications. 
- Be able to effectively use the common neural network "tricks", including initialization, L2 and dropout regularization, Batch normalization, gradient checking, 
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradient descent, Momentum, RMSprop, and Adam, and check for their convergence. 
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets and analyze bias/variance
- Be able to implement a neural network in TensorFlow. 

  Assignments - 

    Week 1 - [(i) Initialization](https://github.com/ajaykumarr28/Deep-Learning-Specialization/blob/master/2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Initialization.ipynb) - In this notebook, I learned how to choose the initialization for a new neural network and saw how different initializations lead to different results.

    Week 1 - [(ii) Regularization](https://github.com/ajaykumarr28/Deep-Learning-Specialization/blob/master/2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Regularization.ipynb) - This notebook showed me how overfitting can be a serious problem and how to use regularization to overcome it.

    Week 1 - [(iii) Gradient Checking](https://github.com/ajaykumarr28/Deep-Learning-Specialization/blob/master/2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Gradient%20Checking.ipynb) - In this notebook, I learned to implement and use gradient checking to get the assurance that backpropagation is working.

    Week 2 - [Optimization Methods](https://github.com/ajaykumarr28/Deep-Learning-Specialization/blob/master/2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Optimization%20methods.ipynb) - In this notebook, I learned more advanced optimization methods that can speed up learning and get me to a better final value for the cost function.

    Week 3 - [Tensorflow Tutorial](https://github.com/ajaykumarr28/Deep-Learning-Specialization/blob/master/2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Tensorflow%20Tutorial.ipynb) - In this assignment, I learned to initialize variables, start my own session, train algorithms, and implement a neural network using TensorFlow.

3. Structuring Machine Learning Projects - This course taught me how to build a successful machine learning project. After completing this course, I was able to:
- Understand how to diagnose errors in a machine learning system, and 
- Be able to prioritize the most promising directions for reducing error
- Understand complex ML settings, such as mismatched training/test sets, and comparing to and/or surpassing human-level performance
- Know how to apply end-to-end learning, transfer learning, and multi-task learning
